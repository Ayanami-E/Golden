# inference.py
"""
å¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„ NPNetV æ¨¡å‹è¿›è¡Œæ¨ç†
"""

import torch
from transformers import CLIPTextModel, CLIPTokenizer

# ç¡®ä¿ os, config, model å­˜åœ¨
import os
import config  # éœ€è¦ config.py æ¥è·å–æ¨¡å‹ç»´åº¦
from model import NPNetV # éœ€è¦ model.py æ¥è·å–æ¨¡å‹æ¶æ„

# --- 1. å®šä¹‰ä½ çš„è¾“å…¥ ---
PROMPT = "a black dog wearing halloween costume" # (ä½ æƒ³ç”Ÿæˆçš„æç¤ºè¯)
MODEL_PATH = "npnet_v_final.pth" # (ä½ åˆšåˆšè®­ç»ƒå¥½çš„æ¨¡å‹)
DEVICE = config.DEVICE


def main():
    if not os.path.exists(MODEL_PATH):
        print(f"é”™è¯¯: æ²¡æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ '{MODEL_PATH}'")
        return

    print("--- 1. åŠ è½½æ¨¡å‹å’Œ Tokenizer ---")
    
    # --- åŠ è½½ CLIP (ç”¨äºç¼–ç æ–‡æœ¬) ---
    # å¿…é¡»ä½¿ç”¨å’Œè®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„ç¼–ç å™¨
    print(f"åŠ è½½æ–‡æœ¬ç¼–ç å™¨: {config.TEXT_ENCODER_MODEL}")
    tokenizer = CLIPTokenizer.from_pretrained(config.TEXT_ENCODER_MODEL)
    text_encoder = CLIPTextModel.from_pretrained(
        config.TEXT_ENCODER_MODEL, 
        use_safetensors=True,
        dtype=torch.float32 # ä¿æŒå’Œè®­ç»ƒæ—¶ä¸€è‡´
    ).to(DEVICE)
    text_encoder.eval()
    
    # --- åŠ è½½ NPNetV (æˆ‘ä»¬çš„å™ªå£°ä¼˜åŒ–å™¨) ---
    print("åŠ è½½ NPNetV æ¨¡å‹æ¶æ„...")
    npnet = NPNetV(
        channels=config.CHANNELS,
        t=config.TEMPORAL_DIM,
        h=config.HEIGHT,
        w=config.WIDTH,
        freq_decay=config.FREQ_DECAY
    ).to(DEVICE)
    
    print(f"åŠ è½½è®­ç»ƒå¥½çš„æƒé‡: {MODEL_PATH}")
    npnet.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    npnet.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (éå¸¸é‡è¦)

    print("\n--- 2. å‡†å¤‡è¾“å…¥ ---")
    
    # --- å‡†å¤‡æ–‡æœ¬åµŒå…¥ E_txt ---
    with torch.no_grad():
        inputs = tokenizer(
            [PROMPT], # æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨ä¸­
            padding="max_length",
            truncation=True,
            max_length=tokenizer.model_max_length,
            return_tensors="pt"
        )
        # E_txt å½¢çŠ¶ (B=1, D=768)
        E_txt = text_encoder(inputs.input_ids.to(DEVICE))[1] 
    
    print(f"æç¤ºè¯ '{PROMPT}' å·²ç¼–ç ä¸º E_txt, å½¢çŠ¶: {E_txt.shape}")

    # --- å‡†å¤‡åˆå§‹é«˜æ–¯å™ªå£° x_T ---
    # (B, C, T, H, W)
    x_T = torch.randn(
        1, # Batch size = 1
        config.CHANNELS,
        config.TEMPORAL_DIM,
        config.HEIGHT,
        config.WIDTH
    ).to(DEVICE)
    
    # (ç¡®ä¿ x_T ä¹Ÿæ˜¯ float32)
    x_T = x_T.float()
    
    print(f"å·²ç”Ÿæˆéšæœºåˆå§‹å™ªå£° x_T, å½¢çŠ¶: {x_T.shape}")

    # --- 3. æ‰§è¡Œå™ªå£°ä¼˜åŒ– ---
    print("\n--- 3. æ­£åœ¨è¿è¡Œ NPNetV... ---")
    
    with torch.no_grad(): # æ¨ç†æ—¶ä¸éœ€è¦æ¢¯åº¦
        x_star_T = npnet(x_T, E_txt)
        
    print("ğŸ‰ æˆåŠŸï¼å·²ç”Ÿæˆä¼˜åŒ–åçš„å™ªå£° x_star_T ğŸ‰")
    print(f"æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {x_star_T.shape}")

    # --- 4. åç»­æ­¥éª¤ ---
    print("\n--- 4. å¦‚ä½•ä½¿ç”¨ ---")
    print("ä½ ç°åœ¨åº”è¯¥å°† 'x_star_T' (è€Œä¸æ˜¯ 'x_T')")
    print("ä½œä¸ºåˆå§‹æ½œå˜é‡ï¼Œè¾“å…¥åˆ°ä½ çš„ T2V æ‰©æ•£æ¨¡å‹ (å¦‚ VideoCrafter2)")
    print("çš„é‡‡æ ·å¾ªç¯ (e.g., DDIM) ä¸­å»ã€‚")

if __name__ == "__main__":
    main()